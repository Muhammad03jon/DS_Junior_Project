import streamlit as st
import pandas as pd
import numpy as np
import string
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from gensim.models.doc2vec import Doc2Vec, TaggedDocument
from sklearn.metrics.pairwise import cosine_similarity

# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏ –º–æ–¥–µ–ª–∏
url = "https://raw.githubusercontent.com/Muhammad03jon/DS_Junior_Project/refs/heads/master/data_for_podcasts.csv"
df_en = pd.read_csv(url)  # –ó–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ –ø—É—Ç—å –∫ –≤–∞—à–µ–º—É —Ñ–∞–π–ª—É

# –ó–∞–≥—Ä—É–∑–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤ NLTK
nltk.download('punkt')
nltk.download('stopwords')

# –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
stop_words = set(stopwords.words('english'))
punctuation = set(string.punctuation)

def preprocess_text(text):
    tokens = word_tokenize(text.lower())  # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
    filtered = [word for word in tokens if word.isalpha() and word not in stop_words and word not in punctuation]
    return filtered

# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ Doc2Vec
def train_doc2vec_model(df_en):
    tagged_data = [
        TaggedDocument(words=preprocess_text(desc), tags=[str(i)])
        for i, desc in enumerate(df_en['description'])
    ]

    model = Doc2Vec(
        vector_size=150,   # –†–∞–∑–º–µ—Ä –≤–µ–∫—Ç–æ—Ä–∞
        window=5,          # –†–∞–∑–º–µ—Ä –æ–∫–Ω–∞
        min_count=2,       # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—Ö–æ–∂–¥–µ–Ω–∏–π —Å–ª–æ–≤–∞
        workers=4,         # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤
        epochs=40,         # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö
        dm=1,              # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å direct context
        hs=0,              # –ù–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–æ–¥–µ–ª–∏ —Å hierarchical softmax
        negative=10        # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –Ω–µ–≥–∞—Ç–∏–≤–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏
    )

    model.build_vocab(tagged_data)
    model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
    model.save("podcast_doc2vec.model")
    return model

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–∞ –ø–æ–¥–∫–∞—Å—Ç–∞
def get_podcast_vector(episode_description, model):
    return model.infer_vector(preprocess_text(episode_description))

# –†–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è
def recommend_similar_podcasts_by_name(episode_name, df_en, model, top_n=20):
    podcast_index = df_en[df_en['episodeName'] == episode_name].index[0]
    target_vector = get_podcast_vector(df_en['description'][podcast_index], model)

    all_vectors = np.array([get_podcast_vector(desc, model) for desc in df_en['description']])

    similarity_scores = cosine_similarity([target_vector], all_vectors)[0]
    similar_indices = similarity_scores.argsort()[-top_n-1:-1][::-1]  # –ò—Å–∫–ª—é—á–∞–µ–º —Å–∞–º –ø–æ–¥–∫–∞—Å—Ç

    results = df_en.iloc[similar_indices][['episodeName', 'show.name', 'show.publisher', 'show.total_episodes']].copy()
    results['model_score'] = similarity_scores[similar_indices].round(2)
    
    return results

# –ì–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ Streamlit
def main():
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏ –º–æ–¥–µ–ª–∏
    df_en = load_data()
    model = train_doc2vec_model(df_en)

    # –°—Ç—Ä–∞–Ω–∏—Ü–∞ –≤—ã–±–æ—Ä–∞
    st.title("Podcast Recommendation System")
    
    page = st.radio("Choose a page:", ("Home", "Project Info"))
    
    if page == "Home":
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ø 20 –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –ø–æ–¥–∫–∞—Å—Ç–æ–≤
        st.subheader("Top 20 Popular Podcasts")
        top_podcasts = df_en.sort_values(by='rank', ascending=False).head(20)

        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∫–∞—Ä—Ç–æ—á–∫–∏ –¥–ª—è —Ç–æ–ø–æ–≤—ã—Ö –ø–æ–¥–∫–∞—Å—Ç–æ–≤
        for index, row in top_podcasts.iterrows():
            if st.button(f"üéß {row['episodeName']}"):
                st.write(f"**Episode Name**: {row['episodeName']}")
                st.write(f"**Show Name**: {row['show.name']}")
                st.write(f"**Publisher**: {row['show.publisher']}")
                st.write(f"**Total Episodes**: {row['show.total_episodes']}")
                
                # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Ö–æ–∂–∏–µ –ø–æ–¥–∫–∞—Å—Ç—ã
                similar_podcasts = recommend_similar_podcasts_by_name(row['episodeName'], df_en, model, top_n=20)
                st.write("### Similar Podcasts:")
                
                for _, similar_row in similar_podcasts.iterrows():
                    st.write(f"**Episode**: {similar_row['episodeName']}")
                    st.write(f"**Show**: {similar_row['show.name']}")
                    st.write(f"**Publisher**: {similar_row['show.publisher']}")
                    st.write(f"**Similarity Score**: {similar_row['model_score']}")
    
    if page == "Project Info":
        st.subheader("About the Project")
        st.write("This project is a Podcast Recommendation System built using Doc2Vec, a technique for converting text to vectors. "
                 "The system recommends similar podcasts based on the descriptions of episodes. Users can view top popular podcasts, "
                 "click on them to see more details, and explore recommendations based on their choices.")
        
        st.write("### Technologies Used:")
        st.write("- **Python**")
        st.write("- **Streamlit** for web interface")
        st.write("- **Doc2Vec** for text vectorization")
        st.write("- **Cosine Similarity** for recommendation")
        st.write("- **Pandas** and **NumPy** for data manipulation")

# –ó–∞–ø—É—Å–∫ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
if __name__ == "__main__":
    main()
